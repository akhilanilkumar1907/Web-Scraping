{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a33b767",
   "metadata": {},
   "source": [
    "The first thing to do will be to scrape review data from the web. For this, you should use a website called Skytrax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ad456",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "---\n",
    "\n",
    "## Web scraping and analysis\n",
    "\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.\n",
    "\n",
    "### Scraping data from Skytrax\n",
    "\n",
    "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc021dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "096156a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "   ---> 100 total reviews\n",
      "Scraping page 2\n",
      "   ---> 200 total reviews\n",
      "Scraping page 3\n",
      "   ---> 300 total reviews\n",
      "Scraping page 4\n",
      "   ---> 400 total reviews\n",
      "Scraping page 5\n",
      "   ---> 500 total reviews\n",
      "Scraping page 6\n",
      "   ---> 600 total reviews\n",
      "Scraping page 7\n",
      "   ---> 700 total reviews\n",
      "Scraping page 8\n",
      "   ---> 800 total reviews\n",
      "Scraping page 9\n",
      "   ---> 900 total reviews\n",
      "Scraping page 10\n",
      "   ---> 1000 total reviews\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "pages = 10\n",
    "page_size = 100\n",
    "\n",
    "reviews = []\n",
    "\n",
    "# for i in range(1, pages + 1):\n",
    "for i in range(1, pages + 1):\n",
    "\n",
    "    print(f\"Scraping page {i}\")\n",
    "\n",
    "    # Create URL to collect links from paginated data\n",
    "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "    # Collect HTML data from this page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse content\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "    for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
    "        reviews.append(para.get_text())\n",
    "    \n",
    "    print(f\"   ---> {len(reviews)} total reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e059d555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>✅ Trip Verified |  My family and I have flown ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>✅ Trip Verified |  This has been by far the wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅ Trip Verified |  In Nov 2022 I booked and pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not Verified | BA is not treating its premium ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✅ Trip Verified |  24 hours before our departu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  ✅ Trip Verified |  My family and I have flown ...\n",
       "1  ✅ Trip Verified |  This has been by far the wo...\n",
       "2  ✅ Trip Verified |  In Nov 2022 I booked and pa...\n",
       "3  Not Verified | BA is not treating its premium ...\n",
       "4  ✅ Trip Verified |  24 hours before our departu..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"reviews\"] = reviews\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e088962",
   "metadata": {},
   "source": [
    "Congratulations! Now you have your dataset for this task! The loops above collected 1000 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!\n",
    "\n",
    " The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f0bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"C:\\Users\\INDHUJA\\Downloads\\CAPSTON PROJECT\\BRITISH AIRWAYS\\DATA\\BA_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dcf5855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>✅ Trip Verified |  My family and I have flown ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>✅ Trip Verified |  This has been by far the wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅ Trip Verified |  In Nov 2022 I booked and pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not Verified | BA is not treating its premium ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✅ Trip Verified |  24 hours before our departu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Not Verified |  Glasgow to Miami via London. G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>✅ Trip Verified |  London Heathrow to Budapest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>✅ Trip Verified |  Budapest to London Heathrow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>✅ Trip Verified |  London to Toronto. Group of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>✅ Trip Verified |  Madrid to Heathrow. I fly w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               reviews\n",
       "0    ✅ Trip Verified |  My family and I have flown ...\n",
       "1    ✅ Trip Verified |  This has been by far the wo...\n",
       "2    ✅ Trip Verified |  In Nov 2022 I booked and pa...\n",
       "3    Not Verified | BA is not treating its premium ...\n",
       "4    ✅ Trip Verified |  24 hours before our departu...\n",
       "..                                                 ...\n",
       "995  Not Verified |  Glasgow to Miami via London. G...\n",
       "996  ✅ Trip Verified |  London Heathrow to Budapest...\n",
       "997  ✅ Trip Verified |  Budapest to London Heathrow...\n",
       "998  ✅ Trip Verified |  London to Toronto. Group of...\n",
       "999  ✅ Trip Verified |  Madrid to Heathrow. I fly w...\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912e116",
   "metadata": {},
   "source": [
    "Removing the parts before | in the reviews column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae98e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reviews=df.reviews.str.split('|',expand=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6386207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I have flown mostly on British...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This has been by far the worst service I hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In Nov 2022 I booked and paid for a return j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BA is not treating its premium economy passen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24 hours before our departure on BA059 to Ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Glasgow to Miami via London. Glasgow to Heat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>London Heathrow to Budapest. After so much d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Budapest to London Heathrow. The flight depa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>London to Toronto. Group of 10 people age fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Madrid to Heathrow. I fly with BA on a regul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               reviews\n",
       "0      My family and I have flown mostly on British...\n",
       "1      This has been by far the worst service I hav...\n",
       "2      In Nov 2022 I booked and paid for a return j...\n",
       "3     BA is not treating its premium economy passen...\n",
       "4      24 hours before our departure on BA059 to Ca...\n",
       "..                                                 ...\n",
       "995    Glasgow to Miami via London. Glasgow to Heat...\n",
       "996    London Heathrow to Budapest. After so much d...\n",
       "997    Budapest to London Heathrow. The flight depa...\n",
       "998    London to Toronto. Group of 10 people age fr...\n",
       "999    Madrid to Heathrow. I fly with BA on a regul...\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ade2bd",
   "metadata": {},
   "source": [
    "*Inference:* We remove the unwanted things from the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f0d70",
   "metadata": {},
   "source": [
    "**How to analyzing text without training or using machine learning models. how to identify which the text is labeled as positive/negative/neutral**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c744bf94",
   "metadata": {},
   "source": [
    "Here are some ways to analyze text without training or using machine learning models to identify which the text is labeled as positive/negative/neutral:\n",
    "\n",
    "**Use sentiment lexicons :** A sentiment lexicon is a list of words that have been assigned a sentiment polarity, such as positive, negative, or neutral. By looking up the words in a text in a sentiment lexicon, you can get a general sense of the overall sentiment of the text.\n",
    "\n",
    "**Use rule-based systems :** Rule-based systems are a set of rules that can be used to determine the sentiment of a text. These rules can be based on the presence of certain words or phrases, the use of negation, or the overall structure of the text.\n",
    "\n",
    "**Data preprocessing steps:**\n",
    "\n",
    "Cleaning the text\n",
    "\n",
    "Tokenization\n",
    "\n",
    "Enrichment – POS tagging\n",
    "\n",
    "Stopwords removal\n",
    "\n",
    "Obtaining the stem words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c178e",
   "metadata": {},
   "source": [
    "# Step 1: Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cbf7c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>Cleaned Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I have flown mostly on British...</td>\n",
       "      <td>my family and i have flown mostly on british a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This has been by far the worst service I hav...</td>\n",
       "      <td>this has been by far the worst service i have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In Nov 2022 I booked and paid for a return j...</td>\n",
       "      <td>in nov  i booked and paid for a return journey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BA is not treating its premium economy passen...</td>\n",
       "      <td>ba is not treating its premium economy passeng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24 hours before our departure on BA059 to Ca...</td>\n",
       "      <td>hours before our departure on ba to cape town ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  \\\n",
       "0    My family and I have flown mostly on British...   \n",
       "1    This has been by far the worst service I hav...   \n",
       "2    In Nov 2022 I booked and paid for a return j...   \n",
       "3   BA is not treating its premium economy passen...   \n",
       "4    24 hours before our departure on BA059 to Ca...   \n",
       "\n",
       "                                     Cleaned Reviews  \n",
       "0  my family and i have flown mostly on british a...  \n",
       "1  this has been by far the worst service i have ...  \n",
       "2  in nov  i booked and paid for a return journey...  \n",
       "3  ba is not treating its premium economy passeng...  \n",
       "4  hours before our departure on ba to cape town ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Define a function to clean the text\n",
    "def clean(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove numbers\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    # Convert to lowercase and remove leading/trailing spaces\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "# Cleaning the text in the review column\n",
    "df['Cleaned Reviews'] = df['reviews'].apply(clean)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7952ccb",
   "metadata": {},
   "source": [
    "# Step 2: Tokenization\n",
    "\n",
    "Tokenization is the process of breaking a text into smaller units, called tokens. These tokens can be words, phrases, or even punctuation marks. Tokenization is a common first step in many natural language processing (NLP) tasks, such as sentiment analysis, machine translation, and text classification.\n",
    "\n",
    "There are two main types of tokenization: word tokenization and sentence tokenization. Word tokenization breaks a text into individual words, while sentence tokenization breaks a text into individual sentences.\n",
    "\n",
    "# Step 3: Enrichment – POS tagging\n",
    "\n",
    "Enrichment – POS tagging is a process of assigning each token in a text a part-of-speech (POS) tag. POS tags are labels that indicate the grammatical category of a word, such as noun, verb, adjective, adverb, preposition, conjunction, or interjection.\n",
    "By knowing the POS tags of the words in a text, it is possible to understand the grammatical structure of the text and to disambiguate words that have multiple meanings.\n",
    "\n",
    "# Step 4: Stopwords removal\n",
    "Stopwords removal is a process of removing common words from a text that do not contribute much to the meaning of the text. Stopwords are words that are very common in a language, such as \"the\", \"and\", \"of\", and \"to\". They are often removed from text before it is analyzed by a natural language processing (NLP) algorithm.\n",
    "\n",
    "# Step 5: Obtaining the stem words\n",
    "A stem is a part of a word responsible for its lexical meaning. The two popular techniques of obtaining the root/stem words are Stemming and Lemmatization.\n",
    "\n",
    "The key difference is Stemming often gives some meaningless root words as it simply chops off some characters in the end. Lemmatization gives meaningful root words, however, it requires POS tags of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a4606",
   "metadata": {},
   "source": [
    "**NLTK** is a leading platform for building Python programs to work with human language data. \n",
    "\n",
    "It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along \n",
    "with a suite of text processing libraries for classification, tokenization, stemming, tagging, \n",
    "parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b1e9733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\INDHUJA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\INDHUJA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\INDHUJA\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\"\"\"This punkt tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, \n",
    "collocations, and words that start sentences. \"\"\"\n",
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33c0ab85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\INDHUJA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\INDHUJA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>Cleaned Reviews</th>\n",
       "      <th>POS tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I have flown mostly on British...</td>\n",
       "      <td>my family and i have flown mostly on british a...</td>\n",
       "      <td>[(family, n), (flown, v), (mostly, r), (britis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This has been by far the worst service I hav...</td>\n",
       "      <td>this has been by far the worst service i have ...</td>\n",
       "      <td>[(far, r), (worst, a), (service, n), (plane, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In Nov 2022 I booked and paid for a return j...</td>\n",
       "      <td>in nov  i booked and paid for a return journey...</td>\n",
       "      <td>[(nov, a), (booked, v), (paid, v), (return, n)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BA is not treating its premium economy passen...</td>\n",
       "      <td>ba is not treating its premium economy passeng...</td>\n",
       "      <td>[(ba, n), (treating, v), (premium, a), (econom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24 hours before our departure on BA059 to Ca...</td>\n",
       "      <td>hours before our departure on ba to cape town ...</td>\n",
       "      <td>[(hours, n), (departure, n), (ba, n), (cape, v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  \\\n",
       "0    My family and I have flown mostly on British...   \n",
       "1    This has been by far the worst service I hav...   \n",
       "2    In Nov 2022 I booked and paid for a return j...   \n",
       "3   BA is not treating its premium economy passen...   \n",
       "4    24 hours before our departure on BA059 to Ca...   \n",
       "\n",
       "                                     Cleaned Reviews  \\\n",
       "0  my family and i have flown mostly on british a...   \n",
       "1  this has been by far the worst service i have ...   \n",
       "2  in nov  i booked and paid for a return journey...   \n",
       "3  ba is not treating its premium economy passeng...   \n",
       "4  hours before our departure on ba to cape town ...   \n",
       "\n",
       "                                          POS tagged  \n",
       "0  [(family, n), (flown, v), (mostly, r), (britis...  \n",
       "1  [(far, r), (worst, a), (service, n), (plane, n...  \n",
       "2  [(nov, a), (booked, v), (paid, v), (return, n)...  \n",
       "3  [(ba, n), (treating, v), (premium, a), (econom...  \n",
       "4  [(hours, n), (departure, n), (ba, n), (cape, v...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The nltk.corpus package defines a collection of corpus reader classes, which can be used to access the contents of a \n",
    "#diverse set of corpora.\n",
    "\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# POS tagger dictionary\n",
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    #print(tags)\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        if word.lower() not in set(stopwords.words('english')):\n",
    "          newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "          #print(tag[0])\n",
    "          #print(pos_dict.get(tag[0]))\n",
    "    return newlist \n",
    "\n",
    "df['POS tagged'] = df['Cleaned Reviews'].apply(token_stop_pos)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40687b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>Cleaned Reviews</th>\n",
       "      <th>POS tagged</th>\n",
       "      <th>Lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I have flown mostly on British...</td>\n",
       "      <td>my family and i have flown mostly on british a...</td>\n",
       "      <td>[(family, n), (flown, v), (mostly, r), (britis...</td>\n",
       "      <td>family fly mostly british airway last year p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This has been by far the worst service I hav...</td>\n",
       "      <td>this has been by far the worst service i have ...</td>\n",
       "      <td>[(far, r), (worst, a), (service, n), (plane, n...</td>\n",
       "      <td>far bad service plane obvious fly economy fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In Nov 2022 I booked and paid for a return j...</td>\n",
       "      <td>in nov  i booked and paid for a return journey...</td>\n",
       "      <td>[(nov, a), (booked, v), (paid, v), (return, n)...</td>\n",
       "      <td>nov book pay return journey new zealand retu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BA is not treating its premium economy passen...</td>\n",
       "      <td>ba is not treating its premium economy passeng...</td>\n",
       "      <td>[(ba, n), (treating, v), (premium, a), (econom...</td>\n",
       "      <td>ba treat premium economy passenger respect p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24 hours before our departure on BA059 to Ca...</td>\n",
       "      <td>hours before our departure on ba to cape town ...</td>\n",
       "      <td>[(hours, n), (departure, n), (ba, n), (cape, v...</td>\n",
       "      <td>hour departure ba cape town heathrow thursda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  \\\n",
       "0    My family and I have flown mostly on British...   \n",
       "1    This has been by far the worst service I hav...   \n",
       "2    In Nov 2022 I booked and paid for a return j...   \n",
       "3   BA is not treating its premium economy passen...   \n",
       "4    24 hours before our departure on BA059 to Ca...   \n",
       "\n",
       "                                     Cleaned Reviews  \\\n",
       "0  my family and i have flown mostly on british a...   \n",
       "1  this has been by far the worst service i have ...   \n",
       "2  in nov  i booked and paid for a return journey...   \n",
       "3  ba is not treating its premium economy passeng...   \n",
       "4  hours before our departure on ba to cape town ...   \n",
       "\n",
       "                                          POS tagged  \\\n",
       "0  [(family, n), (flown, v), (mostly, r), (britis...   \n",
       "1  [(far, r), (worst, a), (service, n), (plane, n...   \n",
       "2  [(nov, a), (booked, v), (paid, v), (return, n)...   \n",
       "3  [(ba, n), (treating, v), (premium, a), (econom...   \n",
       "4  [(hours, n), (departure, n), (ba, n), (cape, v...   \n",
       "\n",
       "                                               Lemma  \n",
       "0    family fly mostly british airway last year p...  \n",
       "1    far bad service plane obvious fly economy fr...  \n",
       "2    nov book pay return journey new zealand retu...  \n",
       "3    ba treat premium economy passenger respect p...  \n",
       "4    hour departure ba cape town heathrow thursda...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining the stem words – Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(pos_data):\n",
    "    lemma_rew = \" \"\n",
    "    for word, pos in pos_data:\n",
    "        if not pos:\n",
    "            lemma = word\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "        else:\n",
    "            lemma = wordnet_lemmatizer.lemmatize(word, pos=pos)\n",
    "            lemma_rew = lemma_rew + \" \" + lemma\n",
    "    return lemma_rew\n",
    "\n",
    "df['Lemma'] = df['POS tagged'].apply(lemmatize)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0473e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>Lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I have flown mostly on British...</td>\n",
       "      <td>family fly mostly british airway last year p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This has been by far the worst service I hav...</td>\n",
       "      <td>far bad service plane obvious fly economy fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In Nov 2022 I booked and paid for a return j...</td>\n",
       "      <td>nov book pay return journey new zealand retu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BA is not treating its premium economy passen...</td>\n",
       "      <td>ba treat premium economy passenger respect p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24 hours before our departure on BA059 to Ca...</td>\n",
       "      <td>hour departure ba cape town heathrow thursda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Glasgow to Miami via London. Glasgow to Heat...</td>\n",
       "      <td>glasgow miami via london glasgow heathrow pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>London Heathrow to Budapest. After so much d...</td>\n",
       "      <td>london heathrow budapest much disappointing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Budapest to London Heathrow. The flight depa...</td>\n",
       "      <td>budapest london heathrow flight depart time ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>London to Toronto. Group of 10 people age fr...</td>\n",
       "      <td>london toronto group people age business cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Madrid to Heathrow. I fly with BA on a regul...</td>\n",
       "      <td>madrid heathrow fly ba regular basis gold ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               reviews  \\\n",
       "0      My family and I have flown mostly on British...   \n",
       "1      This has been by far the worst service I hav...   \n",
       "2      In Nov 2022 I booked and paid for a return j...   \n",
       "3     BA is not treating its premium economy passen...   \n",
       "4      24 hours before our departure on BA059 to Ca...   \n",
       "..                                                 ...   \n",
       "995    Glasgow to Miami via London. Glasgow to Heat...   \n",
       "996    London Heathrow to Budapest. After so much d...   \n",
       "997    Budapest to London Heathrow. The flight depa...   \n",
       "998    London to Toronto. Group of 10 people age fr...   \n",
       "999    Madrid to Heathrow. I fly with BA on a regul...   \n",
       "\n",
       "                                                 Lemma  \n",
       "0      family fly mostly british airway last year p...  \n",
       "1      far bad service plane obvious fly economy fr...  \n",
       "2      nov book pay return journey new zealand retu...  \n",
       "3      ba treat premium economy passenger respect p...  \n",
       "4      hour departure ba cape town heathrow thursda...  \n",
       "..                                                 ...  \n",
       "995    glasgow miami via london glasgow heathrow pa...  \n",
       "996    london heathrow budapest much disappointing ...  \n",
       "997    budapest london heathrow flight depart time ...  \n",
       "998    london toronto group people age business cla...  \n",
       "999    madrid heathrow fly ba regular basis gold ca...  \n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['reviews','Lemma']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436147d9",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562ac727",
   "metadata": {},
   "source": [
    "VADER stands for Valence Aware Dictionary and Sentiment Reasoner.\n",
    "\n",
    "Vader sentiment not only tells if the statement is positive or negative along with the intensity of emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0cc5041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\indhuja\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\indhuja\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\indhuja\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\indhuja\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\indhuja\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.3)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db976267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>Cleaned Reviews</th>\n",
       "      <th>POS tagged</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My family and I have flown mostly on British...</td>\n",
       "      <td>my family and i have flown mostly on british a...</td>\n",
       "      <td>[(family, n), (flown, v), (mostly, r), (britis...</td>\n",
       "      <td>family fly mostly british airway last year p...</td>\n",
       "      <td>0.9422</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This has been by far the worst service I hav...</td>\n",
       "      <td>this has been by far the worst service i have ...</td>\n",
       "      <td>[(far, r), (worst, a), (service, n), (plane, n...</td>\n",
       "      <td>far bad service plane obvious fly economy fr...</td>\n",
       "      <td>0.3287</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In Nov 2022 I booked and paid for a return j...</td>\n",
       "      <td>in nov  i booked and paid for a return journey...</td>\n",
       "      <td>[(nov, a), (booked, v), (paid, v), (return, n)...</td>\n",
       "      <td>nov book pay return journey new zealand retu...</td>\n",
       "      <td>-0.5801</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BA is not treating its premium economy passen...</td>\n",
       "      <td>ba is not treating its premium economy passeng...</td>\n",
       "      <td>[(ba, n), (treating, v), (premium, a), (econom...</td>\n",
       "      <td>ba treat premium economy passenger respect p...</td>\n",
       "      <td>0.6597</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24 hours before our departure on BA059 to Ca...</td>\n",
       "      <td>hours before our departure on ba to cape town ...</td>\n",
       "      <td>[(hours, n), (departure, n), (ba, n), (cape, v...</td>\n",
       "      <td>hour departure ba cape town heathrow thursda...</td>\n",
       "      <td>-0.7713</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  \\\n",
       "0    My family and I have flown mostly on British...   \n",
       "1    This has been by far the worst service I hav...   \n",
       "2    In Nov 2022 I booked and paid for a return j...   \n",
       "3   BA is not treating its premium economy passen...   \n",
       "4    24 hours before our departure on BA059 to Ca...   \n",
       "\n",
       "                                     Cleaned Reviews  \\\n",
       "0  my family and i have flown mostly on british a...   \n",
       "1  this has been by far the worst service i have ...   \n",
       "2  in nov  i booked and paid for a return journey...   \n",
       "3  ba is not treating its premium economy passeng...   \n",
       "4  hours before our departure on ba to cape town ...   \n",
       "\n",
       "                                          POS tagged  \\\n",
       "0  [(family, n), (flown, v), (mostly, r), (britis...   \n",
       "1  [(far, r), (worst, a), (service, n), (plane, n...   \n",
       "2  [(nov, a), (booked, v), (paid, v), (return, n)...   \n",
       "3  [(ba, n), (treating, v), (premium, a), (econom...   \n",
       "4  [(hours, n), (departure, n), (ba, n), (cape, v...   \n",
       "\n",
       "                                               Lemma  Sentiment  Analysis  \n",
       "0    family fly mostly british airway last year p...     0.9422  Positive  \n",
       "1    far bad service plane obvious fly economy fr...     0.3287   Neutral  \n",
       "2    nov book pay return journey new zealand retu...    -0.5801  Negative  \n",
       "3    ba treat premium economy passenger respect p...     0.6597  Positive  \n",
       "4    hour departure ba cape town heathrow thursda...    -0.7713  Negative  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "# function to calculate vader sentiment\n",
    "def vadersentimentanalysis(review):\n",
    "    vs = analyzer.polarity_scores(review)\n",
    "    return vs['compound']\n",
    "\n",
    "df['Sentiment'] = df['Lemma'].apply(vadersentimentanalysis)\n",
    "\n",
    "# function to analyse\n",
    "def vader_analysis(compound):\n",
    "    if compound >= 0.5:\n",
    "        return 'Positive'\n",
    "    elif compound < 0 :\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "df['Analysis'] = df['Sentiment'].apply(vader_analysis)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd5ecb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    488\n",
       "Negative    399\n",
       "Neutral     113\n",
       "Name: Analysis, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_counts = df['Analysis'].value_counts()\n",
    "vader_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af5dd83",
   "metadata": {},
   "source": [
    "# Visual Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c169a97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.patches.Wedge at 0x208dd710040>,\n",
       "  <matplotlib.patches.Wedge at 0x208dd71e730>,\n",
       "  <matplotlib.patches.Wedge at 0x208dd71ee50>],\n",
       " [Text(0.04145918200033701, 1.099218420618879, 'Positive'),\n",
       "  Text(-0.4209517756015994, -1.0162674857624152, 'Negative'),\n",
       "  Text(1.2658227549838803, -0.46924700634635863, 'Neutral')],\n",
       " [Text(0.022614099272911095, 0.599573683973934, '48.8%'),\n",
       "  Text(-0.22961005941905419, -0.5543277195067718, '39.9%'),\n",
       "  Text(0.7969995123972579, -0.29545181881067023, '11.3%')])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(\"Reviews Analysis\")\n",
    "plt.pie(vader_counts.values, labels = vader_counts.index, explode = (0, 0, 0.25), autopct='%1.1f%%', shadow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42dd2eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"BA_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafdc0ef",
   "metadata": {},
   "source": [
    "# Wordcloud\n",
    "\n",
    "A word cloud is a visual representation of text data. It is a cloud-like image of words, where the size of each word corresponds to its frequency in the text. Word clouds are often used to represent the results of text analysis, such as sentiment analysis or topic modeling.\n",
    "\n",
    "Word clouds are created by first tokenizing the text into individual words. Then, the frequency of each word is calculated. The size of each word in the word cloud is then proportional to its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dc361b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.2-cp39-cp39-win_amd64.whl (153 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\indhuja\\anaconda3\\lib\\site-packages (from wordcloud) (9.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\indhuja\\anaconda3\\lib\\site-packages (from wordcloud) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\indhuja\\anaconda3\\lib\\site-packages (from wordcloud) (1.21.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\indhuja\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\indhuja\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\indhuja\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\indhuja\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\indhuja\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\indhuja\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\indhuja\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66f0e949",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only supported for TrueType fonts",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(wordcloud)\n\u001b[0;32m     19\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 21\u001b[0m \u001b[43mshow_wordcloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLemma\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36mshow_wordcloud\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_wordcloud\u001b[39m(data):\n\u001b[0;32m      5\u001b[0m     wordcloud \u001b[38;5;241m=\u001b[39m WordCloud(\n\u001b[0;32m      6\u001b[0m         background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m         stopwords\u001b[38;5;241m=\u001b[39mstopwords,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m         scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     11\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     wordcloud\u001b[38;5;241m=\u001b[39m\u001b[43mwordcloud\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(\u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m12\u001b[39m))\n\u001b[0;32m     16\u001b[0m     plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:639\u001b[0m, in \u001b[0;36mWordCloud.generate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \n\u001b[0;32m    627\u001b[0m \u001b[38;5;124;03m    The input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:621\u001b[0m, in \u001b[0;36mWordCloud.generate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;124;03m\"\"\"Generate wordcloud from text.\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03mThe input \"text\" is expected to be a natural text. If you pass a sorted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;124;03mself\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    620\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_text(text)\n\u001b[1;32m--> 621\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_from_frequencies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\wordcloud\\wordcloud.py:508\u001b[0m, in \u001b[0;36mWordCloud.generate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    505\u001b[0m transposed_font \u001b[38;5;241m=\u001b[39m ImageFont\u001b[38;5;241m.\u001b[39mTransposedFont(\n\u001b[0;32m    506\u001b[0m     font, orientation\u001b[38;5;241m=\u001b[39morientation)\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# get size of resulting text\u001b[39;00m\n\u001b[1;32m--> 508\u001b[0m box_size \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtextbbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfont\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransposed_font\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# find possible places using integral image:\u001b[39;00m\n\u001b[0;32m    510\u001b[0m result \u001b[38;5;241m=\u001b[39m occupancy\u001b[38;5;241m.\u001b[39msample_position(box_size[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmargin,\n\u001b[0;32m    511\u001b[0m                                    box_size[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmargin,\n\u001b[0;32m    512\u001b[0m                                    random_state)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\ImageDraw.py:671\u001b[0m, in \u001b[0;36mImageDraw.textbbox\u001b[1;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[0;32m    669\u001b[0m     font \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetfont()\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(font, ImageFont\u001b[38;5;241m.\u001b[39mFreeTypeFont):\n\u001b[1;32m--> 671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly supported for TrueType fonts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    672\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGBA\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m embedded_color \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfontmode\n\u001b[0;32m    673\u001b[0m bbox \u001b[38;5;241m=\u001b[39m font\u001b[38;5;241m.\u001b[39mgetbbox(\n\u001b[0;32m    674\u001b[0m     text, mode, direction, features, language, stroke_width, anchor\n\u001b[0;32m    675\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Only supported for TrueType fonts"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "def show_wordcloud(data):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=100,\n",
    "        max_font_size=30,\n",
    "        scale=3,\n",
    "        random_state=1)\n",
    "\n",
    "    wordcloud=wordcloud.generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "\n",
    "show_wordcloud(df.Lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c418aa2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ab57fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66716bdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a4dcdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc713209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f7d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26426356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2953dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad951494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18edf563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de20da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b7fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b2d896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e43b15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80eaeab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836f642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72a605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f83f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b98da58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df69b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ae30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332bbb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f2398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f88c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891a04aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae11b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15254376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72059f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f203cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f220370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6e7e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba0e5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d9ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b523b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11384eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf3b93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d73627",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa907ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0868c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043d7c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06e2091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ae930",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef9b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6708e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d4955f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0b049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb750285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9329e02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bee8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b82bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf03eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f3a07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3145fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505518c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a0add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0e189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36592207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962b8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0591136f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ea0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb1b19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf9146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd6338b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73012daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ee239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd743bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30bb301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35487ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf8413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd5e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db48298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb98dc4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb3d6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd51f016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e30c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
